üè¶ Open Banking API Billing Data System (OBABS) - Design & Resilience1. Executive Summary and RequirementsThis document outlines the proposed architecture for the Open Banking API Billing Data System (OBABS), designed to handle high-volume, low-latency ingestion of financial event data. Based on requirements, the recommended architecture is Option 2 (DB-First), prioritizing real-time accuracy and simplified operational control during the active billing period.ComponentRequirementDetailVolumeHigh-throughput ingestion$\approx$ 2 Billion calls/month ($\approx 770$ events/sec sustained).EventsGranularityMust process API Calls (Apigee) and Consent Events (Consent System).AttributionParty LinkageMust link events to the 2nd, 3rd, and 4th (Billing) parties for invoicing.RetentionStorage Tiers14 months in fast, active storage (MongoDB); archive to cold storage (S3) thereafter.ToolingConstraintSystem must be built using only Kafka, MongoDB, and S3-like Object Storage.2. Proposed Solution OptionsOption 1: Kafka $\rightarrow$ Object Storage $\rightarrow$ MongoDB (Lake-First)In this approach, raw events are immediately dumped to cheap S3 storage via a Kafka Sink. A complex Batch Processor (ETL job) is then required to read the entire dataset (14 months), enrich the data, and UPSERT the resulting records into MongoDB for active querying. While cost-effective on the MongoDB side, this introduces high complexity and latency in the core data transformation pipeline.Option 2: Kafka $\rightarrow$ MongoDB $\rightarrow$ Archive to Object Storage (DB-First) [RECOMMENDED]This model uses a custom Billing Ingest Service (Kafka Consumer) to read events in real-time, enrich them with pricing and party IDs, and insert them directly into a highly-sharded MongoDB cluster. MongoDB serves as the single source of truth for all active (14-month) billing data. Older data is proactively migrated to S3 via a dedicated Archival Job.Justification for Option 2: For a financial billing system, minimizing data latency and ensuring real-time data integrity (idempotency, unique keys) is non-negotiable. Option 2 provides immediate queryability for invoice generation and simplified state enforcement.3. Recommended System Architecture (Option 2)A. Data Flow OverviewEvent Source: Apigee/Consent System publishes raw events to Kafka.Ingestion & Enrichment: The Billing Ingest Service (Kafka Consumer) reads the raw topic, queries the pricing_master and account_master collections in MongoDB for enrichment, applies idempotency checks, and inserts the final billable record.Active Storage: The event lands in the highly sharded MongoDB Cluster for the current and prior 13 months of billing.Archival: A monthly Archival Job queries MongoDB for records older than 14 months, exports them to Parquet files in S3, and safely deletes them from MongoDB.B. MongoDB Sharding StrategyDue to the massive volume ($2$ Billion events/month) and the time-series nature of the data, the core billing collection must be sharded to prevent hot-spotting and enable horizontal scale.CollectionShard KeyJustificationbillable_events{ "billing_party_id": 1, "timestamp_utc": 1 }Ensures all events for a single customer are co-located, optimizing targeted query performance for monthly invoice generation.pricing_masterUnsharded / Simple Primary IndexSmall, frequently read collection. Faster as a single replica set.4. Exceptional Scenarios & Mitigation StrategyMitigating the following edge cases is critical to guarantee data accuracy and system resilience.ScenarioComponent AffectedDescriptionMitigation StrategyLate Events (Active)Ingestion / BillingEvent arrives with a timestamp_utc up to 14 months old.Insertion is allowed. Billing reports must always rely on the event's timestamp_utc for inclusion in the correct invoice period.Late Events (Archived)Billing / ArchivalEvent arrives with a timestamp_utc older than the 14-month retention window.Correction Mechanism: The event is written to a special late_corrections MongoDB collection. A dedicated job exports this as a correction file (with negative/positive values) to the relevant S3 partition for adjustment in the next open invoice.Duplicate EventsIngestion / MongoDBA single API call event is published twice due to producer retry logic.Idempotency Check: The Ingest Service must enforce a Unique Index on the event's identifier (event_id or composite key) in the MongoDB collection. Duplicates are silently discarded or logged via a unique index violation.Schema DriftIngestionA core event field changes format or is removed by the producer system (Apigee).Dead Letter Queue (DLQ): Messages that fail parsing, validation, or enrichment are moved to a separate DLQ Kafka topic. This prevents the consumer from crashing and allows manual investigation/reprocessing.Missing Pricing DataEnrichmentAn event is valid, but the associated pricing_master rate cannot be found.Default to Zero: The enrichment service assigns a chargeable_rate of $0.00 and inserts the record with a flag needs_review: true. This prevents overcharging and allows billing to proceed while alerting operations to configure the correct rate immediately.Event Reversal/RefundIngestion / BillingA previously billed event needs to be cancelled or refunded (e.g., fraudulent call).REVERSAL Event: The producer system generates a new REVERSAL event referencing the original event_id. The Ingest Service inserts a new record with a negative chargeable_rate to effectively offset the original transaction.Archival Job FailureArchivalThe batch export job fails after writing to S3 but before deleting from MongoDB.Near-Transactional Archival: The job must implement a two-step process: 1) Validate the S3 file integrity (checksum/record count). 2) Only upon success, proceed to safe deletion from MongoDB. The job must be idempotent on restart.5. Cost & Storage CommentaryA. Storage RequirementsTierDurationVolume (Estimate)Cost DriverActive (MongoDB)14 Months$\approx \mathbf{18.2 \text{ TB}}$ (Includes indexes)High-cost, high-IOPS cluster.Archived (S3)5 Years+$\approx \mathbf{60 \text{ TB}}$ (Parquet compressed)Low-cost, durable storage tier.B. Principal Cost DriverThe primary operational cost will be the MongoDB Sharded Cluster. To handle $\approx 770$ writes per second and maintain low-latency query performance across a dataset approaching $20 \text{ TB}$, a high-availability, high-performance configuration with dedicated compute and storage resources is required. This demands careful monitoring, indexing, and sharding configuration to control costs.
